{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# 加载预训练的 YOLOv11n 模型\n",
    "model_path = '/home/szh/work/Weed_Detection/ultralytics-main/yolo11n.pt'\n",
    "model = YOLO(model_path)\n",
    "source = '/home/szh/work/Weed_Detection/cat.jpg' #更改为自己的图片路径\n",
    "# 运行推理，并附加参数\n",
    "model.predict(source, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集格式转yolo格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "# 定义目标label的类ID\n",
    "LABELS = {\n",
    "    \"weed\": 0,  # 其他所有的 label 统一为 \"weed\"，class_id 为 0\n",
    "    \"mq\": 1     # mq 对应的 class_id 为 1\n",
    "}\n",
    "\n",
    "def convert_to_yolo_format(points, img_width, img_height):\n",
    "    # YOLO格式: class_id center_x center_y width height\n",
    "    # points 是一个包含两个坐标 [(x1, y1), (x2, y2)]，表示的是矩形的两个点\n",
    "    x_1, y_1 = points[0]\n",
    "    x_2, y_2 = points[1]\n",
    "\n",
    "    r = math.sqrt((x_2 - x_1) ** 2 + (y_2 - y_1) ** 2)\n",
    "    # 外接正方形的左上角和右下角\n",
    "    x1 = x_1 - r\n",
    "    y1 = y_1 - r\n",
    "    \n",
    "    x2 = x_1 + r\n",
    "    y2 = y_1 + r\n",
    "\n",
    "\n",
    "    # 将矩形的两个点转换为边界框的中心点和宽高\n",
    "    center_x = (x1 + x2) / 2.0 / img_width\n",
    "    center_y = (y1 + y2) / 2.0 / img_height\n",
    "\n",
    "    width = (2*r) / img_width\n",
    "    height = (2*r) / img_height\n",
    "\n",
    "    return center_x, center_y, width, height\n",
    "\n",
    "def process_json_label(json_file, output_dir):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    img_width = data['imageWidth']\n",
    "    img_height = data['imageHeight']\n",
    "    \n",
    "    # 获取图像的文件名 (无后缀)\n",
    "    image_filename = os.path.splitext(os.path.basename(data['imagePath']))[0]\n",
    "\n",
    "    # 输出的YOLO格式标签文件路径\n",
    "    yolo_txt_path = os.path.join(output_dir, f\"{image_filename}.txt\")\n",
    "    \n",
    "    # 打开文件写入YOLO格式的标签\n",
    "    with open(yolo_txt_path, 'w') as yolo_file:\n",
    "        for shape in data['shapes']:\n",
    "            label = shape['label']\n",
    "            \n",
    "            # 只检测 \"mq\"，其余的都统一成 \"weed\"\n",
    "            if label == 'mq':\n",
    "                class_id = LABELS['mq']\n",
    "            else:\n",
    "                class_id = LABELS['weed']\n",
    "            \n",
    "            # 解析 points，转换成 YOLO 格式\n",
    "            points = shape['points']\n",
    "            center_x, center_y, width, height = convert_to_yolo_format(points, img_width, img_height)\n",
    "            \n",
    "            # 写入文件，格式: class_id center_x center_y width height\n",
    "            yolo_file.write(f\"{class_id} {center_x} {center_y} {width} {height}\\n\")\n",
    "\n",
    "def convert_dataset(json_dir, output_dir):\n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 遍历所有的json文件\n",
    "    for filename in os.listdir(json_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            json_file = os.path.join(json_dir, filename)\n",
    "            process_json_label(json_file, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 输入的JSON标注文件目录\n",
    "    json_dir = \"/home/szh/work/Weed_Detection/train/labels\"\n",
    "    # 输出的YOLO格式标签的目录\n",
    "    output_dir = \"/home/szh/work/Weed_Detection/train/labels_process\"\n",
    "    \n",
    "    convert_dataset(json_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练--5折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from ultralytics import YOLO\n",
    "\n",
    "NUM_THREADS = min(8, max(1, os.cpu_count() - 1))\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--data', default='/home/szh/work/Weed_Detection/train')  # 数据集路径\n",
    "    parser.add_argument('--ksplit', default=5, type=int)  # K-Fold交叉验证拆分数据集\n",
    "    parser.add_argument('--im_suffixes', default=['jpg', 'png', 'jpeg'], help='images suffix')  # 图片后缀名\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def run(func, this_iter, desc=\"Processing\"):\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREADS, thread_name_prefix='MyThread') as executor:\n",
    "        results = list(\n",
    "            tqdm(executor.map(func, this_iter), total=len(this_iter), desc=desc)\n",
    "        )\n",
    "    return results\n",
    "\n",
    "def main(opt):\n",
    "    dataset_path, ksplit, im_suffixes = Path(opt.data), opt.ksplit, opt.im_suffixes\n",
    "\n",
    "    save_path = Path(dataset_path / f'{datetime.date.today().isoformat()}_{ksplit}-Fold_Cross-Valid')\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 获取所有图像和标签文件的列表\n",
    "    images = sorted(list(chain(*[(dataset_path / \"images\").rglob(f'*.{ext}') for ext in im_suffixes])))\n",
    "    # images = sorted(image_files)\n",
    "    labels = sorted((dataset_path / \"labels\").rglob(\"*.txt\"))\n",
    "\n",
    "    root_directory = Path.cwd()\n",
    "    print(\"当前文件运行根目录:\", root_directory)\n",
    "    if len(images) != len(labels):\n",
    "        print('*' * 20)\n",
    "        print('当前数据集和标签数量不一致！！！')\n",
    "        print('*' * 20)\n",
    "\n",
    "    # 从YAML文件加载类名\n",
    "    yaml_file = '/home/szh/work/Weed_Detection/ultralytics-main/classes.yaml'\n",
    "    with open(yaml_file, 'r', encoding=\"utf8\") as y:\n",
    "        classes = yaml.safe_load(y)['names']\n",
    "    cls_idx = sorted(classes.keys())\n",
    "\n",
    "    # classes_file = sorted(dataset_path.rglob('classes.yaml'))[0]\n",
    "    # assert classes_file.exists(), \"请创建classes.yaml类别文件\"\n",
    "    # if classes_file.suffix == \".txt\":\n",
    "    #     pass\n",
    "    # elif classes_file.suffix == \".yaml\":\n",
    "    #     with open(classes_file, 'r', encoding=\"utf8\") as f:\n",
    "    #         classes = yaml.safe_load(f)['names']\n",
    "    # cls_idx = sorted(classes.keys())\n",
    "\n",
    "    # 创建DataFrame来存储每张图像的标签计数\n",
    "    indx = [l.stem for l in labels]  # 使用基本文件名作为ID（无扩展名）\n",
    "    labels_df = pd.DataFrame([], columns=cls_idx, index=indx)\n",
    "\n",
    "    # 计算每张图像的标签计数\n",
    "    for label in labels:\n",
    "        lbl_counter = Counter()\n",
    "        with open(label, 'r') as lf:\n",
    "            lines = lf.readlines()\n",
    "        for l in lines:\n",
    "            # YOLO标签使用每行的第一个位置的整数作为类别\n",
    "            lbl_counter[int(l.split(' ')[0])] += 1\n",
    "        labels_df.loc[label.stem] = lbl_counter\n",
    "\n",
    "    # 用0.0替换NaN值\n",
    "    labels_df = labels_df.fillna(0.0)\n",
    "\n",
    "    kf = KFold(n_splits=ksplit, shuffle=True, random_state=20)  # 设置random_state以获得可重复的结果\n",
    "    kfolds = list(kf.split(labels_df))\n",
    "    folds = [f'split_{n}' for n in range(1, ksplit + 1)]\n",
    "    folds_df = pd.DataFrame(index=indx, columns=folds)\n",
    "\n",
    "    # 为每个折叠分配图像到训练集或验证集\n",
    "    for idx, (train, val) in enumerate(kfolds, start=1):\n",
    "        folds_df[f'split_{idx}'].loc[labels_df.iloc[train].index] = 'train'\n",
    "        folds_df[f'split_{idx}'].loc[labels_df.iloc[val].index] = 'val'\n",
    "\n",
    "    # 计算每个折叠的标签分布比例\n",
    "    fold_lbl_distrb = pd.DataFrame(index=folds, columns=cls_idx)\n",
    "    for n, (train_indices, val_indices) in enumerate(kfolds, start=1):\n",
    "        train_totals = labels_df.iloc[train_indices].sum()\n",
    "        val_totals = labels_df.iloc[val_indices].sum()\n",
    "\n",
    "        # 为避免分母为零，向分母添加一个小值（1E-7）\n",
    "        ratio = val_totals / (train_totals + 1E-7)\n",
    "        fold_lbl_distrb.loc[f'split_{n}'] = ratio\n",
    "\n",
    "    ds_yamls = []\n",
    "\n",
    "    for split in folds_df.columns:\n",
    "        split_dir = save_path / split\n",
    "        split_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (split_dir / 'train' / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (split_dir / 'train' / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "        (split_dir / 'val' / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (split_dir / 'val' / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        dataset_yaml = split_dir / f'{split}_dataset.yaml'\n",
    "        ds_yamls.append(dataset_yaml.as_posix())\n",
    "        split_dir = (root_directory / split_dir).as_posix()\n",
    "\n",
    "        with open(dataset_yaml, 'w') as ds_y:\n",
    "            yaml.safe_dump({\n",
    "                'train': split_dir + '/train/images',\n",
    "                'val': split_dir + '/val/images',\n",
    "                'names': classes\n",
    "            }, ds_y)\n",
    "    # print(ds_yamls)\n",
    "    with open(dataset_path / 'yaml_paths.txt', 'w') as f:\n",
    "        for path in ds_yamls:\n",
    "            f.write(path + '\\n')\n",
    "\n",
    "    args_list = [(image, save_path, folds_df) for image in images]\n",
    "\n",
    "    run(split_images_labels, args_list, desc=f\"Creating dataset\")\n",
    "\n",
    "def split_images_labels(args):\n",
    "    image, save_path, folds_df = args\n",
    "    label = image.parents[1] / 'labels' / f'{image.stem}.txt'\n",
    "    if label.exists():\n",
    "        for split, k_split in folds_df.loc[image.stem].items():\n",
    "            # 目标目录\n",
    "            img_to_path = save_path / split / k_split / 'images'\n",
    "            lbl_to_path = save_path / split / k_split / 'labels'\n",
    "            shutil.copy(image, img_to_path / image.name)\n",
    "            shutil.copy(label, lbl_to_path / label.name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n",
    "\n",
    "    model = YOLO('ultralytics/cfg/models/11/yolo11s.yaml')  # 从YAML建立一个新模型\n",
    "    model_path = '/home/szh/work/Weed_Detection/ultralytics-main/yolo11s.pt'\n",
    "    model.load(model_path)\n",
    "    \n",
    "    # 从文本文件中加载内容并存储到一个列表中\n",
    "    ds_yamls = []\n",
    "    with open(Path(opt.data) / 'yaml_paths.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            # 去除每行末尾的换行符\n",
    "            line = line.strip()\n",
    "            ds_yamls.append(line)\n",
    "\n",
    "    # 打印加载的文件路径列表\n",
    "    print(ds_yamls)\n",
    "\n",
    "    for k in range(opt.ksplit):\n",
    "        dataset_yaml = ds_yamls[k]\n",
    "        name = Path(dataset_yaml).stem\n",
    "        model.train(data=dataset_yaml,name=name,epochs=100,patience=15,imgsz=640, device=[0,1], optimizer='SGD', workers=8, batch=64, amp=False, iou = 0.5,cos_lr= True)\n",
    "\n",
    "    print(\"*\"*40)\n",
    "    print(\"K-Fold Cross Validation Completed.\")\n",
    "    print(\"*\"*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件的数据\n",
    "# 这里假设 result.csv 文件存放在你的工作目录下，或者提供文件的绝对路径\n",
    "file_path = '/home/szh/work/Weed_Detection/ultralytics-main/runs/detect/split_1_dataset/results.csv'\n",
    "\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 绘制训练和验证损失\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 创建一个图，绘制训练和验证损\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['epoch'], df['train/box_loss'], label='train/box_loss', color='blue')\n",
    "plt.plot(df['epoch'], df['val/box_loss'], label='val/box_loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Box Loss')\n",
    "plt.title('Box Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['epoch'], df['train/cls_loss'], label='train/cls_loss', color='blue')\n",
    "plt.plot(df['epoch'], df['val/cls_loss'], label='val/cls_loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Class Loss')\n",
    "plt.title('Class Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['epoch'], df['train/dfl_loss'], label='train/dfl_loss', color='blue')\n",
    "plt.plot(df['epoch'], df['val/dfl_loss'], label='val/dfl_loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('DFL Loss')\n",
    "plt.title('DFL Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制其他指标（Precision, Recall, mAP50等）\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['epoch'], df['metrics/precision(B)'], label='Precision', color='green')\n",
    "plt.plot(df['epoch'], df['metrics/recall(B)'], label='Recall', color='orange')\n",
    "plt.plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP50', color='purple')\n",
    "plt.plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP50-95', color='brown')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Precision, Recall, mAP')\n",
    "plt.legend()\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pdb\n",
    "import torch\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# torch.cuda.set_device(3)\n",
    "\n",
    "# 加载训练好的模型\n",
    "best_model_path = '/home/szh/work/Weed_Detection/ultralytics-main/runs/detect/split_4_dataset/weights/best.pt'\n",
    "model = YOLO(best_model_path)\n",
    "\n",
    "# 测试图像目录\n",
    "source_dir = '/home/szh/work/Weed_Detection/test/images'  # 修改为自己的图片路径\n",
    "\n",
    "# 获取目录下所有图像文件\n",
    "image_files = [f for f in os.listdir(source_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(len(image_files))\n",
    "\n",
    "# 准备保存的列表\n",
    "output_data = []\n",
    "\n",
    "# 遍历每个图像文件并进行推理\n",
    "j=0\n",
    "for image_file in image_files:\n",
    "    # 提取文件名作为 image_id（去掉扩展名）\n",
    "    image_id = os.path.splitext(image_file)[0]\n",
    "    print(\"image_id = \",image_id)\n",
    "\n",
    "    # 生成图像的完整路径\n",
    "    image_path = os.path.join(source_dir, image_file)\n",
    "    \n",
    "    # 运行推理\n",
    "    results = model.predict(image_path)\n",
    "    \n",
    "    # 遍历推理结果\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # 获取预测到的所有框\n",
    "        for i, box in enumerate(boxes):\n",
    "            class_id = int(box.cls.item())  # 类别: 0 为 'weed', 1 为 'mq'\n",
    "            \n",
    "            # 获取 xyxy 坐标\n",
    "            xyxy = box.xyxy[0].cpu().numpy()  # 将 Tensor 转换为 NumPy 数组\n",
    "            \n",
    "            # 提取坐标\n",
    "            x_min = int(xyxy[0])  # 左上角 X 坐标\n",
    "            y_min = int(xyxy[1])  # 左上角 Y 坐标\n",
    "            x_max = int(xyxy[2])  # 右下角 X 坐标\n",
    "            y_max = int(xyxy[3])  # 右下角 Y 坐标\n",
    "            \n",
    "            # 计算宽度和高度\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            \n",
    "            # 添加到输出数据\n",
    "            j=j+1\n",
    "            output_data.append([j, image_id, class_id, x_min, y_min, width, height])\n",
    "            \n",
    "\n",
    "# 将数据保存为 CSV 格式\n",
    "df = pd.DataFrame(output_data, columns=['ID', 'image_id', 'class_id', 'x_min', 'y_min', 'width', 'height'])\n",
    "df.to_csv('/home/szh/work/Weed_Detection/test/output_predictions_split_4.csv', index=False)\n",
    "\n",
    "print(\"预测结果已保存到 '/home/szh/work/Weed_Detection/test/output_predictions_split_4.csv'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
